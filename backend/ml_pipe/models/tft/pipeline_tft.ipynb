{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from  lightning.pytorch import Trainer\n",
    "from  lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from  lightning.pytorch.loggers import TensorBoardLogger\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/')\n",
    "\n",
    "from backend.ml_pipe.data.dataModule.tft.dataModule import CareerDataModule\n",
    "from backend.ml_pipe.models.tft.model import TFTModel\n",
    "from  lightning.pytorch.loggers import CSVLogger\n",
    "from pytorch_forecasting import TimeSeriesDataSet, GroupNormalizer, TemporalFusionTransformer, QuantileLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.ml_pipe.data.database.mongodb import MongoDb\n",
    "\n",
    "def load_data_from_mongodb():\n",
    "    mongo_client = MongoDb(user='florianrunkel', password='ur04mathesis', db_name='Database')\n",
    "    result = mongo_client.get_all('time_dataset')\n",
    "    raw_data = result.get('data', [])\n",
    "    # In DataFrame umwandeln\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(raw_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Karriere-Vorhersage Pipeline...\n",
      "Lade Daten aus MongoDB...\n",
      "Train: 50294 Val: 24954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/data/timeseries.py:1831: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 4486 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__profile_id': '%C4%85%C5%BEuolas-kru%C5%A1na-85761583'}, {'__group_id__profile_id': '13-sabrina-sauter'}, {'__group_id__profile_id': '13748178'}, {'__group_id__profile_id': '32475a292'}, {'__group_id__profile_id': '3614427'}, {'__group_id__profile_id': '689543254'}, {'__group_id__profile_id': 'a-gerold'}, {'__group_id__profile_id': 'a-lemar-omar-68b503159'}, {'__group_id__profile_id': 'a-pyeshchyk'}, {'__group_id__profile_id': 'a-schirpfer'}]\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/data/timeseries.py:1831: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 5170 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__profile_id': '%C4%85%C5%BEuolas-kru%C5%A1na-85761583'}, {'__group_id__profile_id': '13-sabrina-sauter'}, {'__group_id__profile_id': '13748178'}, {'__group_id__profile_id': '32475a292'}, {'__group_id__profile_id': '3614427'}, {'__group_id__profile_id': '689543254'}, {'__group_id__profile_id': 'a-gerold'}, {'__group_id__profile_id': 'a-lemar-omar-68b503159'}, {'__group_id__profile_id': 'a-pyeshchyk'}, {'__group_id__profile_id': 'a-schirpfer'}]\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 50294 Kandidaten\n",
      "Validation: 24954 Kandidaten\n",
      "Initialisiere TFT Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 100.2k\n",
      "Starte Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 7.9 K  | train\n",
      "3  | prescalers                         | ModuleDict                      | 416    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 6.5 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 20.6 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 11.9 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 231    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "100 K     Trainable params\n",
      "0         Non-trainable params\n",
      "100 K     Total params\n",
      "0.401     Total estimated model params size (MB)\n",
      "450       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 98/98 [00:16<00:00,  5.95it/s, v_num=0, train_loss_step=75.00, val_loss=110.0, train_loss_epoch=84.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 98/98 [00:16<00:00,  5.91it/s, v_num=0, train_loss_step=75.00, val_loss=110.0, train_loss_epoch=84.40]\n",
      "Bestes Modell: /Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/checkpoints/tft-best-epoch=00-val_loss=109.81.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.]], device='mps:0')\n",
      "\n",
      "Modell-Evaluation auf Validierungsdaten:\n",
      "MAE: 244.80 Tage\n",
      "RMSE: 432.87 Tage\n",
      "Trainingsdataset gespeichert unter: training_dataset.pt\n",
      "Speichere Modell...\n",
      "Modell gespeichert unter: ./saved_models/tft_20250520_154626.ckpt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from  lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from  lightning.pytorch.loggers import TensorBoardLogger\n",
    "import  lightning.pytorch as pl\n",
    "import os\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "\n",
    "def run_pipeline():\n",
    "    print(\"Starte Karriere-Vorhersage Pipeline...\")\n",
    "\n",
    "    # 1. Daten laden\n",
    "    print(\"Lade Daten aus MongoDB...\")\n",
    "    df = load_data_from_mongodb()\n",
    "\n",
    "    # 2. DataModule initialisieren\n",
    "    max_encoder_length = 30\n",
    "    max_prediction_length = 7\n",
    "    datamodule = CareerDataModule(df, batch_size=64, max_encoder_length=max_encoder_length, max_prediction_length=max_prediction_length)\n",
    "    datamodule.setup()\n",
    "    print(f\"Training: {len(datamodule.training)} Kandidaten\")\n",
    "    print(f\"Validation: {len(datamodule.validation)} Kandidaten\")\n",
    "\n",
    "    # 3. Modell initialisieren\n",
    "    print(\"Initialisiere TFT Modell...\")\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        datamodule.training_dataset,\n",
    "        learning_rate=0.03,\n",
    "        hidden_size=32,\n",
    "        attention_head_size=2,\n",
    "        dropout=0.1,\n",
    "        hidden_continuous_size=16,\n",
    "        output_size=7,  # Quantile output\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {model.size()/1e3:.1f}k\")\n",
    "\n",
    "    # 4. Trainer initialisieren\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=1e-4,\n",
    "        patience=10,\n",
    "        verbose=False,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "\n",
    "    lr_logger = LearningRateMonitor()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"tft-best-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(save_dir=\"logs\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=1,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "        gradient_clip_val=0.1,\n",
    "        log_every_n_steps=10,\n",
    "        callbacks=[lr_logger, checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # 5. Training starten\n",
    "    print(\"Starte Training...\")\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=datamodule.train_dataloader(),\n",
    "        val_dataloaders=datamodule.val_dataloader(),\n",
    "    )\n",
    "\n",
    "    # 6. Bestes Modell laden\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    print(f\"Bestes Modell: {best_model_path}\")\n",
    "\n",
    "    best_model = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "    # Evaluation auf Validierungsdaten\n",
    "    val_dataloader = datamodule.val_dataloader()\n",
    "    \n",
    "    # Vorhersagen und tatsächliche Werte sammeln\n",
    "    actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "    predictions = best_model.predict(val_dataloader)\n",
    "    print(predictions)\n",
    "\n",
    "    # Alle Tensoren auf dasselbe Gerät bringen\n",
    "    actuals = actuals.to(predictions.device)\n",
    "\n",
    "    # Metriken initialisieren\n",
    "    from pytorch_forecasting.metrics import MAE, RMSE\n",
    "    mae_metric = MAE().to(predictions.device)\n",
    "    rmse_metric = RMSE().to(predictions.device)\n",
    "\n",
    "    # Metriken berechnen\n",
    "    mae = mae_metric(predictions, actuals)\n",
    "    rmse = rmse_metric(predictions, actuals)\n",
    "\n",
    "    print(\"\\nModell-Evaluation auf Validierungsdaten:\")\n",
    "    print(f\"MAE: {mae:.2f} Tage\")\n",
    "    print(f\"RMSE: {rmse:.2f} Tage\")\n",
    "\n",
    "    # Speichere das Trainingsdataset\n",
    "    torch.save(datamodule.training_dataset, \"/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/training_dataset.pt\")\n",
    "    print(\"Trainingsdataset gespeichert unter: training_dataset.pt\")\n",
    "\n",
    "    # 9. Modell speichern\n",
    "    print(\"Speichere Modell...\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = f\"./saved_models/tft_{timestamp}.ckpt\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    trainer.save_checkpoint(model_path)\n",
    "    print(f\"Modell gespeichert unter: {model_path}\")\n",
    "\n",
    "    return best_model, trainer\n",
    "\n",
    "# Pipeline ausführen\n",
    "best_model, trainer = run_pipeline()\n",
    "#[I 2025-05-20 16:25:48,382] Trial 0 finished with value: 137.7418975830078 and parameters: {'gradient_clip_val': 0.7663094383604092, 'hidden_size': 63, 'dropout': 0.27868811302673885, 'hidden_continuous_size': 9, 'attention_head_size': 3, 'learning_rate': 0.011199776102729224}. Best is trial 0 with value: 137.7418975830078.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/data/timeseries.py:1831: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 4486 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__profile_id': '%C4%85%C5%BEuolas-kru%C5%A1na-85761583'}, {'__group_id__profile_id': '13-sabrina-sauter'}, {'__group_id__profile_id': '13748178'}, {'__group_id__profile_id': '32475a292'}, {'__group_id__profile_id': '3614427'}, {'__group_id__profile_id': '689543254'}, {'__group_id__profile_id': 'a-gerold'}, {'__group_id__profile_id': 'a-lemar-omar-68b503159'}, {'__group_id__profile_id': 'a-pyeshchyk'}, {'__group_id__profile_id': 'a-schirpfer'}]\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/data/timeseries.py:1831: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 5170 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__profile_id': '%C4%85%C5%BEuolas-kru%C5%A1na-85761583'}, {'__group_id__profile_id': '13-sabrina-sauter'}, {'__group_id__profile_id': '13748178'}, {'__group_id__profile_id': '32475a292'}, {'__group_id__profile_id': '3614427'}, {'__group_id__profile_id': '689543254'}, {'__group_id__profile_id': 'a-gerold'}, {'__group_id__profile_id': 'a-lemar-omar-68b503159'}, {'__group_id__profile_id': 'a-pyeshchyk'}, {'__group_id__profile_id': 'a-schirpfer'}]\n",
      "  warnings.warn(\n",
      "[I 2025-05-20 15:57:33,370] A new study created in memory with name: no-name-88d72b06-427b-4a66-b4d8-38c4c3290661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 50294 Val: 24954\n",
      "Training: 50294 Kandidaten\n",
      "Validation: 24954 Kandidaten\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py:142: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  gradient_clip_val = trial.suggest_loguniform(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py:168: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout=trial.suggest_uniform(\"dropout\", *dropout_range),\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py:222: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  model.hparams.learning_rate = trial.suggest_loguniform(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-05-20 16:25:48,382] Trial 0 finished with value: 137.7418975830078 and parameters: {'gradient_clip_val': 0.7663094383604092, 'hidden_size': 63, 'dropout': 0.27868811302673885, 'hidden_continuous_size': 9, 'attention_head_size': 3, 'learning_rate': 0.011199776102729224}. Best is trial 0 with value: 137.7418975830078.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py:142: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  gradient_clip_val = trial.suggest_loguniform(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py:168: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout=trial.suggest_uniform(\"dropout\", *dropout_range),\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/tuning.py:222: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  model.hparams.learning_rate = trial.suggest_loguniform(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "df = load_data_from_mongodb()\n",
    "\n",
    "datamodule = CareerDataModule(df, batch_size=64, max_encoder_length=30, max_prediction_length=7)\n",
    "datamodule.setup()\n",
    "print(f\"Training: {len(datamodule.training)} Kandidaten\")\n",
    "print(f\"Validation: {len(datamodule.validation)} Kandidaten\")\n",
    "\n",
    "# Optuna-Studie für Hyperparameter-Tuning starten\n",
    "study = optimize_hyperparameters(\n",
    "    datamodule.train_dataloader(),\n",
    "    datamodule.val_dataloader(),\n",
    "    model_path=\"optuna_test\",  # Hier werden die besten Modelle gespeichert\n",
    "    n_trials=200,              # Anzahl der Versuche (Random Search/Bayesian)\n",
    "    max_epochs=50,             # Maximale Epochen pro Versuch\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),  # Optional: schnelleres Tuning\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,\n",
    ")\n",
    "\n",
    "# Studie speichern, um später fortzusetzen\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# Beste Hyperparameter anzeigen\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hw/wbg7dg7934z5jd1dv8g7rxjc0000gn/T/ipykernel_22780/3114539223.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training_dataset = torch.load(\"/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/training_dataset.pt\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/tft_20250515_153035.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 137\u001b[0m\n\u001b[1;32m    128\u001b[0m prediction_dataset \u001b[38;5;241m=\u001b[39m TimeSeriesDataSet\u001b[38;5;241m.\u001b[39mfrom_dataset(\n\u001b[1;32m    129\u001b[0m     training_dataset,\n\u001b[1;32m    130\u001b[0m     df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     target_normalizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# 7. Modell laden\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTemporalFusionTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/tft_20250515_153035.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# 8. Vorhersage machen\u001b[39;00m\n\u001b[1;32m    140\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m prediction_dataset\u001b[38;5;241m.\u001b[39mto_dataloader(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/core/module.py:1582\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1501\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \n\u001b[1;32m   1581\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1582\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/core/saving.py:63\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m map_location \u001b[38;5;241m=\u001b[39m map_location \u001b[38;5;129;01mor\u001b[39;00m _default_map_location\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 63\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     66\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     67\u001b[0m     checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/fabric/utilities/cloud_io.py:59\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location, weights_only)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[1;32m     55\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m     61\u001b[0m         f,\n\u001b[1;32m     62\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[1;32m     64\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fsspec/spec.py:1295\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1295\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fsspec/implementations/local.py:180\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fsspec/implementations/local.py:302\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fsspec/implementations/local.py:307\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    309\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/tft_20250515_153035.ckpt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "import json\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# 1. Trainingsdataset laden\n",
    "training_dataset = torch.load(\"/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/training_dataset.pt\")\n",
    "\n",
    "# 2. Position Mapping laden\n",
    "with open(\"/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/data/featureEngineering/position_level.json\", \"r\") as f:\n",
    "    position_entries = json.load(f)\n",
    "\n",
    "# Dict: Position (klein) -> (Level, Branche)\n",
    "position_map = {entry[\"position\"].lower(): (entry[\"level\"], entry[\"branche\"]) for entry in position_entries}\n",
    "all_positions = list(position_map.keys())\n",
    "\n",
    "# Level-Zahl auf String\n",
    "level_map = {\n",
    "    1: \"Entry\", 2: \"Junior\", 3: \"Professional\", 4: \"Senior\", 5: \"Lead\", 6: \"Manager\", 7: \"Director\", 8: \"C-Level\"\n",
    "}\n",
    "\n",
    "def map_position_fuzzy(pos, threshold=65):\n",
    "    pos_clean = pos.lower().strip()\n",
    "    if pos_clean in position_map:\n",
    "        level, branche = position_map[pos_clean]\n",
    "        match = pos_clean\n",
    "        score = 100  # Maximale Ähnlichkeit, da exakter Treffer\n",
    "    else:\n",
    "        match, score, _ = process.extractOne(pos_clean, all_positions, scorer=fuzz.ratio)\n",
    "        if score >= threshold:\n",
    "            level, branche = position_map[match]\n",
    "        else:\n",
    "            return (None, None, None)\n",
    "    # Level-Zahl auf String mappen\n",
    "    level_str = level_map.get(level, str(level)) if isinstance(level, int) else str(level)\n",
    "    return (match, level_str, branche)\n",
    "\n",
    "# 3. LinkedIn-Profil verarbeiten\n",
    "profile_data = {\n",
    "    \"workExperience\": [\n",
    "        {\n",
    "            \"position\": \"Senior Software Engineer\",\n",
    "            \"company\": \"Tech Corp\",\n",
    "            \"startDate\": \"01/01/2023\",\n",
    "            \"endDate\": \"Present\"\n",
    "        },\n",
    "        {\n",
    "            \"position\": \"Software Engineer\",\n",
    "            \"company\": \"Startup GmbH\",\n",
    "            \"startDate\": \"01/01/2021\",\n",
    "            \"endDate\": \"31/12/2022\"\n",
    "        },\n",
    "        {\n",
    "            \"position\": \"Junior Developer\",\n",
    "            \"company\": \"IT Solutions\",\n",
    "            \"startDate\": \"01/01/2020\",\n",
    "            \"endDate\": \"31/12/2020\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 4. Daten für Vorhersage vorbereiten\n",
    "time_points = []\n",
    "experiences = sorted(\n",
    "    profile_data['workExperience'],\n",
    "    key=lambda x: datetime.strptime(x['startDate'], \"%d/%m/%Y\"),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for i, exp in enumerate(experiences):\n",
    "    start_date = datetime.strptime(exp['startDate'], \"%d/%m/%Y\")\n",
    "    end_date = datetime.now() if exp['endDate'] == \"Present\" else datetime.strptime(exp['endDate'], \"%d/%m/%Y\")\n",
    "    \n",
    "    # Position mappen\n",
    "    mapped_pos, level_str, branche = map_position_fuzzy(exp['position'])\n",
    "    if mapped_pos is None:\n",
    "        print(f\"Warnung: Position '{exp['position']}' konnte nicht gemappt werden\")\n",
    "        continue\n",
    "    \n",
    "    # Erstelle 8 Zeitpunkte pro Position\n",
    "    for j in range(8):\n",
    "        timepoint = start_date + timedelta(days=int((end_date - start_date).days * (j + 1) / 8))\n",
    "        \n",
    "        # Berechne Features\n",
    "        berufserfahrung = (timepoint - datetime.strptime(experiences[-1]['startDate'], \"%d/%m/%Y\")).days\n",
    "        anzahl_wechsel = sum(1 for e in experiences if e['endDate'] != \"Present\" and datetime.strptime(e['endDate'], \"%d/%m/%Y\") <= timepoint)\n",
    "        anzahl_jobs = sum(1 for e in experiences if datetime.strptime(e['startDate'], \"%d/%m/%Y\") <= timepoint)\n",
    "        \n",
    "        # Berechne durchschnittliche Jobdauer\n",
    "        dauer_liste = []\n",
    "        for e in experiences:\n",
    "            s = datetime.strptime(e['startDate'], \"%d/%m/%Y\")\n",
    "            e_date = datetime.now() if e['endDate'] == \"Present\" else datetime.strptime(e['endDate'], \"%d/%m/%Y\")\n",
    "            if s < e_date and e_date <= timepoint:\n",
    "                dauer_liste.append((e_date - s).days)\n",
    "        durchschnittsdauer = sum(dauer_liste) / len(dauer_liste) if dauer_liste else 0\n",
    "        \n",
    "        # Erstelle DataFrame-Zeile\n",
    "        row = {\n",
    "            \"profile_id\": \"predict_profile\",\n",
    "            \"time_idx\": i * 8 + j,\n",
    "            \"label\": 0,  # Wird vorhergesagt\n",
    "            \"berufserfahrung_bis_zeitpunkt\": berufserfahrung,\n",
    "            \"anzahl_wechsel_bisher\": anzahl_wechsel,\n",
    "            \"anzahl_jobs_bisher\": anzahl_jobs,\n",
    "            \"durchschnittsdauer_bisheriger_jobs\": durchschnittsdauer,\n",
    "            \"zeitpunkt\": timepoint.timestamp(),\n",
    "            \"aktuelle_position\": exp['position'],\n",
    "            \"mapped_position\": mapped_pos,\n",
    "            \"level_str\": level_str,\n",
    "            \"branche\": branche,\n",
    "            \"weekday\": timepoint.weekday(),\n",
    "            \"weekday_sin\": np.sin(2 * np.pi * timepoint.weekday() / 7),\n",
    "            \"weekday_cos\": np.cos(2 * np.pi * timepoint.weekday() / 7),\n",
    "            \"month\": timepoint.month,\n",
    "            \"month_sin\": np.sin(2 * np.pi * timepoint.month / 12),\n",
    "            \"month_cos\": np.cos(2 * np.pi * timepoint.month / 12)\n",
    "        }\n",
    "        time_points.append(row)\n",
    "\n",
    "# 5. DataFrame erstellen\n",
    "df = pd.DataFrame(time_points)\n",
    "\n",
    "# 6. Vorhersage-Dataset erstellen\n",
    "prediction_dataset = TimeSeriesDataSet.from_dataset(\n",
    "    training_dataset,\n",
    "    df,\n",
    "    predict=True,\n",
    "    stop_randomization=True,\n",
    "    target_normalizer=None \n",
    ")\n",
    "\n",
    "# 7. Modell laden\n",
    "model = TemporalFusionTransformer.load_from_checkpoint(\"/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/tft_20250515_153035.ckpt\")\n",
    "\n",
    "# 8. Vorhersage machen\n",
    "dataloader = prediction_dataset.to_dataloader(train=False, batch_size=1)\n",
    "predictions = model.predict(dataloader)\n",
    "\n",
    "# 9. Ergebnisse ausgeben\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Tag {i+1}: {float(pred[0]):.2f} Tage bis zum nächsten Jobwechsel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'workExperience': [{'position': 'Disponentin', 'company': 'M-net Telekommunikations GmbH', 'startDate': '03/2023', 'endDate': 'Present'}, {'position': 'Sales Development Representative', 'company': 'Personio', 'startDate': '08/2022', 'endDate': '12/2022'}, {'position': 'Sales Consultant', 'company': 'Bolia.com', 'startDate': '11/2017', 'endDate': '07/2022'}, {'position': 'Senior Team Lead', 'company': 'Pepperminds', 'startDate': '10/2015', 'endDate': '11/2017'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/predict.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5210, 0.6829, 0.4316, 0.4626, 1.0129, 0.7703, 0.5844]],\n",
      "       device='mps:0')\n",
      "{'confidence': [0.5209989547729492], 'recommendations': ['Sehr wahrscheinlicher Jobwechsel innerhalb des nächsten Monats'], 'status': 'success', 'explanations': ['Basierend auf Ihrer Berufserfahrung wird ein Jobwechsel in etwa 0.0 Monaten erwartet.', 'Die Vorhersage basiert auf dem Medianwert der Modellprognose.'], 'predictions': [{'tag': 1, 'vorhersage': {'median': 0.5209989547729492, 'untere_schranke': 0.6828992962837219, 'obere_schranke': 0.7703423500061035, 'unsicherheit': 0.08744305372238159}}]}\n",
      "confidence\n",
      "recommendations\n",
      "status\n",
      "explanations\n",
      "predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "from predict import read_linkedin_profile, predict\n",
    "\n",
    "# LinkedIn-Profil als String\n",
    "linkedin_data_str = r'''{\"skills\":[\"Multitasking\",\"Kundenservice\",\"Interpersonelle Fähigkeiten\",\"Kaltakquise\",\"Hubspot CRM\",\"Customer-Relationship-Management (CRM)\"],\"firstName\":\"Darya\",\"lastName\":\"Chernuska\",\"profilePicture\":\"https://media.licdn.com/dms/image/v2/D4E03AQE0yuZ6cg8f4A/profile-displayphoto-shrink_100_100/profile-displayphoto-shrink_100_100/0/1670856025914?e=1749686400&v=beta&t=jI1mkiVnkD7teWPncsg8QtKAwZKB-az53_4ny7C7XvI\",\"linkedinProfile\":\"https://www.linkedin.com/in/daryachernuska\",\"education\":[{\"duration\":\"01/01/2017 - 01/01/2022\",\"institution\":\"Ludwig-Maximilians-Universität München\",\"endDate\":\"01/01/2022\",\"degree\":\"\",\"startDate\":\"01/01/2017\"}],\"providerId\":\"ACoAAD0rz_IBI0XfqqBDUscwHoFwuOqJa_c5T2I\",\"workExperience\":[{\"duration\":\"01/03/2023 - Present\",\"endDate\":\"Present\",\"companyInformation\":{\"employee_count\":515,\"activities\":[\"Telefonie\",\"Internet\",\"Vernetzung\",\"Rechenzentrum\",\"Glasfaser\",\"Highspeed-Internet\",\"Business-Internet\",\"SIP-Trunk\",\"Cloud-Lösungen\",\"Connect-Cloud\",\"Connect-LAN\",\"Premium IP\",\"Internet + Telefonie\",\"Lösungen für Geschäftskunden\"],\"name\":\"M-net Telekommunikations GmbH\",\"description\":\"Als regionaler Telekommunikationsanbieter versorgt M-net große Teile Bayerns, den Großraum Ulm sowie weite Teile des hessischen Landkreises Main-Kinzig mit zukunftssicherer Kommunikationstechnologie.\",\"industry\":[\"Telecommunications\"]},\"description\":\"\",\"company\":\"M-net Telekommunikations GmbH\",\"location\":\"München, Bayern, Deutschland · Hybrid\",\"position\":\"Disponentin\",\"startDate\":\"01/03/2023\"},{\"duration\":\"01/08/2022 - 01/12/2022\",\"endDate\":\"01/12/2022\",\"companyInformation\":{\"employee_count\":2048,\"activities\":[\"HR Software\",\"HR Management\",\"Recruitung\",\"Employee Management\",\"Applicant Tracking System\",\"Employee Selfservice\",\"Time-Off Management\",\"Cloud Software\",\"Onboarding and Offboarding\",\"HR Reporting\",\"Performance Management\",\"Payroll\",\"HR\",\"HR Tech\",\"Human Resources\"],\"name\":\"Personio\",\"description\":\"Personio's Intelligent HR Platform helps small and medium-sized organizations unlock the power of people by making complicated, time-consuming tasks simple and efficient.\",\"industry\":[\"Software Development\"]},\"description\":\"\",\"company\":\"Personio\",\"location\":\"München, Bayern, Deutschland\",\"position\":\"Sales Development Representative\",\"startDate\":\"01/08/2022\"},{\"duration\":\"01/11/2017 - 01/07/2022\",\"endDate\":\"01/07/2022\",\"companyInformation\":{\"employee_count\":662,\"activities\":[\"Scandinavian design\",\"Furniture\",\"Design\",\"Product design\",\"Retail\",\"Web\",\"Steelcase partner\",\"Wholesale\",\"B2B\",\"Contract sales\",\"Online\",\"Digital\",\"Creativity\"],\"name\":\"BOLIA\",\"description\":\"Our collection is inspired by the vivid Scandinavian nature\",\"industry\":[\"Retail Furniture and Home Furnishings\"]},\"description\":\"\",\"company\":\"Bolia.com\",\"location\":\"München, Bayern, Deutschland\",\"position\":\"Sales Consultant\",\"startDate\":\"01/11/2017\"},{\"duration\":\"01/10/2015 - 01/11/2017\",\"endDate\":\"01/11/2017\",\"companyInformation\":{},\"description\":\"\",\"company\":\"Pepperminds\",\"location\":\"München, Bayern, Deutschland\",\"position\":\"Senior Team Lead\",\"startDate\":\"01/10/2015\"}],\"location\":\"Munich, Bavaria, Germany\",\"certifications\":[],\"headline\":\"-\",\"languageSkills\":{}}'''\n",
    "\n",
    "# Profil einlesen\n",
    "profile_data = read_linkedin_profile(linkedin_data_str)\n",
    "print(profile_data)\n",
    "\n",
    "if profile_data:\n",
    "    # Vorhersage machen\n",
    "    predictions = predict(profile_data)\n",
    "    print(predictions)\n",
    "    \n",
    "    if predictions:\n",
    "        # Ergebnisse ausgeben\n",
    "        for pred in predictions:\n",
    "            print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
