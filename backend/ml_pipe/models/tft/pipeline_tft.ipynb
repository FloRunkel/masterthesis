{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from  lightning.pytorch import Trainer\n",
    "from  lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from  lightning.pytorch.loggers import TensorBoardLogger\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/')\n",
    "\n",
    "from backend.ml_pipe.data.dataModule.tft.dataModule import CareerDataModule\n",
    "from backend.ml_pipe.models.tft.model import TFTModel\n",
    "from  lightning.pytorch.loggers import CSVLogger\n",
    "from pytorch_forecasting import TimeSeriesDataSet, GroupNormalizer, TemporalFusionTransformer, QuantileLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.ml_pipe.data.database.mongodb import MongoDb\n",
    "\n",
    "def load_data_from_mongodb():\n",
    "    mongo_client = MongoDb(user='florianrunkel', password='ur04mathesis', db_name='Database')\n",
    "    result = mongo_client.get_all('time_dataset')\n",
    "    raw_data = result.get('data', [])\n",
    "    # In DataFrame umwandeln\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(raw_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Karriere-Vorhersage Pipeline...\n",
      "Lade Daten aus MongoDB...\n",
      "Train: 50294 Val: 24954\n",
      "Training: 50294 Kandidaten\n",
      "Validation: 24954 Kandidaten\n",
      "Initialisiere TFT Modell...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/data/timeseries.py:1831: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 4486 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__profile_id': '%C4%85%C5%BEuolas-kru%C5%A1na-85761583'}, {'__group_id__profile_id': '13-sabrina-sauter'}, {'__group_id__profile_id': '13748178'}, {'__group_id__profile_id': '32475a292'}, {'__group_id__profile_id': '3614427'}, {'__group_id__profile_id': '689543254'}, {'__group_id__profile_id': 'a-gerold'}, {'__group_id__profile_id': 'a-lemar-omar-68b503159'}, {'__group_id__profile_id': 'a-pyeshchyk'}, {'__group_id__profile_id': 'a-schirpfer'}]\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytorch_forecasting/data/timeseries.py:1831: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 5170 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__profile_id': '%C4%85%C5%BEuolas-kru%C5%A1na-85761583'}, {'__group_id__profile_id': '13-sabrina-sauter'}, {'__group_id__profile_id': '13748178'}, {'__group_id__profile_id': '32475a292'}, {'__group_id__profile_id': '3614427'}, {'__group_id__profile_id': '689543254'}, {'__group_id__profile_id': 'a-gerold'}, {'__group_id__profile_id': 'a-lemar-omar-68b503159'}, {'__group_id__profile_id': 'a-pyeshchyk'}, {'__group_id__profile_id': 'a-schirpfer'}]\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/checkpoints exists and is not empty.\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 7.9 K  | train\n",
      "3  | prescalers                         | ModuleDict                      | 416    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 6.5 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 20.6 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 11.9 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 231    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "100 K     Trainable params\n",
      "0         Non-trainable params\n",
      "100 K     Total params\n",
      "0.401     Total estimated model params size (MB)\n",
      "450       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 100.2k\n",
      "Starte Training...\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 98/98 [00:27<00:00,  3.61it/s, v_num=28, train_loss_step=52.30, val_loss=122.0, train_loss_epoch=52.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 98/98 [00:27<00:00,  3.60it/s, v_num=28, train_loss_step=52.30, val_loss=122.0, train_loss_epoch=52.30]\n",
      "Bestes Modell: /Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/checkpoints/tft-best-epoch=01-val_loss=99.52.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modell-Evaluation auf Validierungsdaten:\n",
      "MAE: 190.76 Tage\n",
      "RMSE: 368.66 Tage\n",
      "Trainingsdataset gespeichert unter: training_dataset.pt\n",
      "Speichere Modell...\n",
      "Modell gespeichert unter: ./saved_models/tft_20250515_163355.ckpt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from  lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from  lightning.pytorch.loggers import TensorBoardLogger\n",
    "import  lightning.pytorch as pl\n",
    "import os\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "\n",
    "def run_pipeline():\n",
    "    print(\"Starte Karriere-Vorhersage Pipeline...\")\n",
    "\n",
    "    # 1. Daten laden\n",
    "    print(\"Lade Daten aus MongoDB...\")\n",
    "    df = load_data_from_mongodb()\n",
    "\n",
    "    # 2. DataModule initialisieren\n",
    "    max_encoder_length = 30\n",
    "    max_prediction_length = 7\n",
    "    datamodule = CareerDataModule(df, batch_size=64, max_encoder_length=max_encoder_length, max_prediction_length=max_prediction_length)\n",
    "    datamodule.setup()\n",
    "    print(f\"Training: {len(datamodule.training)} Kandidaten\")\n",
    "    print(f\"Validation: {len(datamodule.validation)} Kandidaten\")\n",
    "\n",
    "    # 3. Modell initialisieren\n",
    "    print(\"Initialisiere TFT Modell...\")\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        datamodule.training_dataset,\n",
    "        learning_rate=0.03,\n",
    "        hidden_size=32,\n",
    "        attention_head_size=2,\n",
    "        dropout=0.1,\n",
    "        hidden_continuous_size=16,\n",
    "        output_size=7,  # Quantile output\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {model.size()/1e3:.1f}k\")\n",
    "\n",
    "    # 4. Trainer initialisieren\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=1e-4,\n",
    "        patience=10,\n",
    "        verbose=False,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "\n",
    "    lr_logger = LearningRateMonitor()\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"tft-best-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(save_dir=\"logs\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=50,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "        gradient_clip_val=0.1,\n",
    "        log_every_n_steps=10,\n",
    "        callbacks=[lr_logger, checkpoint_callback],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # 5. Training starten\n",
    "    print(\"Starte Training...\")\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=datamodule.train_dataloader(),\n",
    "        val_dataloaders=datamodule.val_dataloader(),\n",
    "    )\n",
    "\n",
    "    # 6. Bestes Modell laden\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    print(f\"Bestes Modell: {best_model_path}\")\n",
    "\n",
    "    best_model = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "    # Evaluation auf Validierungsdaten\n",
    "    val_dataloader = datamodule.val_dataloader()\n",
    "    \n",
    "    # Vorhersagen und tatsächliche Werte sammeln\n",
    "    actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "    predictions = best_model.predict(val_dataloader)\n",
    "\n",
    "    # Alle Tensoren auf dasselbe Gerät bringen\n",
    "    actuals = actuals.to(predictions.device)\n",
    "\n",
    "    # Metriken initialisieren\n",
    "    from pytorch_forecasting.metrics import MAE, RMSE\n",
    "    mae_metric = MAE().to(predictions.device)\n",
    "    rmse_metric = RMSE().to(predictions.device)\n",
    "\n",
    "    # Metriken berechnen\n",
    "    mae = mae_metric(predictions, actuals)\n",
    "    rmse = rmse_metric(predictions, actuals)\n",
    "\n",
    "    print(\"\\nModell-Evaluation auf Validierungsdaten:\")\n",
    "    print(f\"MAE: {mae:.2f} Tage\")\n",
    "    print(f\"RMSE: {rmse:.2f} Tage\")\n",
    "\n",
    "    # Speichere das Trainingsdataset\n",
    "    torch.save(datamodule.training_dataset, \"/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/training_dataset.pt\")\n",
    "    print(\"Trainingsdataset gespeichert unter: training_dataset.pt\")\n",
    "\n",
    "    # 9. Modell speichern\n",
    "    print(\"Speichere Modell...\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_path = f\"./saved_models/tft_{timestamp}.ckpt\"\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    trainer.save_checkpoint(model_path)\n",
    "    print(f\"Modell gespeichert unter: {model_path}\")\n",
    "\n",
    "    return best_model, trainer\n",
    "\n",
    "# Pipeline ausführen\n",
    "best_model, trainer = run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hw/wbg7dg7934z5jd1dv8g7rxjc0000gn/T/ipykernel_29748/3114539223.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training_dataset = torch.load(\"/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/training_dataset.pt\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag 1: 0.50 Tage bis zum nächsten Jobwechsel\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "import json\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# 1. Trainingsdataset laden\n",
    "training_dataset = torch.load(\"/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/training_dataset.pt\")\n",
    "\n",
    "# 2. Position Mapping laden\n",
    "with open(\"/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/data/featureEngineering/position_level.json\", \"r\") as f:\n",
    "    position_entries = json.load(f)\n",
    "\n",
    "# Dict: Position (klein) -> (Level, Branche)\n",
    "position_map = {entry[\"position\"].lower(): (entry[\"level\"], entry[\"branche\"]) for entry in position_entries}\n",
    "all_positions = list(position_map.keys())\n",
    "\n",
    "# Level-Zahl auf String\n",
    "level_map = {\n",
    "    1: \"Entry\", 2: \"Junior\", 3: \"Professional\", 4: \"Senior\", 5: \"Lead\", 6: \"Manager\", 7: \"Director\", 8: \"C-Level\"\n",
    "}\n",
    "\n",
    "def map_position_fuzzy(pos, threshold=65):\n",
    "    pos_clean = pos.lower().strip()\n",
    "    if pos_clean in position_map:\n",
    "        level, branche = position_map[pos_clean]\n",
    "        match = pos_clean\n",
    "        score = 100  # Maximale Ähnlichkeit, da exakter Treffer\n",
    "    else:\n",
    "        match, score, _ = process.extractOne(pos_clean, all_positions, scorer=fuzz.ratio)\n",
    "        if score >= threshold:\n",
    "            level, branche = position_map[match]\n",
    "        else:\n",
    "            return (None, None, None)\n",
    "    # Level-Zahl auf String mappen\n",
    "    level_str = level_map.get(level, str(level)) if isinstance(level, int) else str(level)\n",
    "    return (match, level_str, branche)\n",
    "\n",
    "# 3. LinkedIn-Profil verarbeiten\n",
    "profile_data = {\n",
    "    \"workExperience\": [\n",
    "        {\n",
    "            \"position\": \"Senior Software Engineer\",\n",
    "            \"company\": \"Tech Corp\",\n",
    "            \"startDate\": \"01/01/2023\",\n",
    "            \"endDate\": \"Present\"\n",
    "        },\n",
    "        {\n",
    "            \"position\": \"Software Engineer\",\n",
    "            \"company\": \"Startup GmbH\",\n",
    "            \"startDate\": \"01/01/2021\",\n",
    "            \"endDate\": \"31/12/2022\"\n",
    "        },\n",
    "        {\n",
    "            \"position\": \"Junior Developer\",\n",
    "            \"company\": \"IT Solutions\",\n",
    "            \"startDate\": \"01/01/2020\",\n",
    "            \"endDate\": \"31/12/2020\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 4. Daten für Vorhersage vorbereiten\n",
    "time_points = []\n",
    "experiences = sorted(\n",
    "    profile_data['workExperience'],\n",
    "    key=lambda x: datetime.strptime(x['startDate'], \"%d/%m/%Y\"),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for i, exp in enumerate(experiences):\n",
    "    start_date = datetime.strptime(exp['startDate'], \"%d/%m/%Y\")\n",
    "    end_date = datetime.now() if exp['endDate'] == \"Present\" else datetime.strptime(exp['endDate'], \"%d/%m/%Y\")\n",
    "    \n",
    "    # Position mappen\n",
    "    mapped_pos, level_str, branche = map_position_fuzzy(exp['position'])\n",
    "    if mapped_pos is None:\n",
    "        print(f\"Warnung: Position '{exp['position']}' konnte nicht gemappt werden\")\n",
    "        continue\n",
    "    \n",
    "    # Erstelle 8 Zeitpunkte pro Position\n",
    "    for j in range(8):\n",
    "        timepoint = start_date + timedelta(days=int((end_date - start_date).days * (j + 1) / 8))\n",
    "        \n",
    "        # Berechne Features\n",
    "        berufserfahrung = (timepoint - datetime.strptime(experiences[-1]['startDate'], \"%d/%m/%Y\")).days\n",
    "        anzahl_wechsel = sum(1 for e in experiences if e['endDate'] != \"Present\" and datetime.strptime(e['endDate'], \"%d/%m/%Y\") <= timepoint)\n",
    "        anzahl_jobs = sum(1 for e in experiences if datetime.strptime(e['startDate'], \"%d/%m/%Y\") <= timepoint)\n",
    "        \n",
    "        # Berechne durchschnittliche Jobdauer\n",
    "        dauer_liste = []\n",
    "        for e in experiences:\n",
    "            s = datetime.strptime(e['startDate'], \"%d/%m/%Y\")\n",
    "            e_date = datetime.now() if e['endDate'] == \"Present\" else datetime.strptime(e['endDate'], \"%d/%m/%Y\")\n",
    "            if s < e_date and e_date <= timepoint:\n",
    "                dauer_liste.append((e_date - s).days)\n",
    "        durchschnittsdauer = sum(dauer_liste) / len(dauer_liste) if dauer_liste else 0\n",
    "        \n",
    "        # Erstelle DataFrame-Zeile\n",
    "        row = {\n",
    "            \"profile_id\": \"predict_profile\",\n",
    "            \"time_idx\": i * 8 + j,\n",
    "            \"label\": 0,  # Wird vorhergesagt\n",
    "            \"berufserfahrung_bis_zeitpunkt\": berufserfahrung,\n",
    "            \"anzahl_wechsel_bisher\": anzahl_wechsel,\n",
    "            \"anzahl_jobs_bisher\": anzahl_jobs,\n",
    "            \"durchschnittsdauer_bisheriger_jobs\": durchschnittsdauer,\n",
    "            \"zeitpunkt\": timepoint.timestamp(),\n",
    "            \"aktuelle_position\": exp['position'],\n",
    "            \"mapped_position\": mapped_pos,\n",
    "            \"level_str\": level_str,\n",
    "            \"branche\": branche,\n",
    "            \"weekday\": timepoint.weekday(),\n",
    "            \"weekday_sin\": np.sin(2 * np.pi * timepoint.weekday() / 7),\n",
    "            \"weekday_cos\": np.cos(2 * np.pi * timepoint.weekday() / 7),\n",
    "            \"month\": timepoint.month,\n",
    "            \"month_sin\": np.sin(2 * np.pi * timepoint.month / 12),\n",
    "            \"month_cos\": np.cos(2 * np.pi * timepoint.month / 12)\n",
    "        }\n",
    "        time_points.append(row)\n",
    "\n",
    "# 5. DataFrame erstellen\n",
    "df = pd.DataFrame(time_points)\n",
    "\n",
    "# 6. Vorhersage-Dataset erstellen\n",
    "prediction_dataset = TimeSeriesDataSet.from_dataset(\n",
    "    training_dataset,\n",
    "    df,\n",
    "    predict=True,\n",
    "    stop_randomization=True,\n",
    "    target_normalizer=None \n",
    ")\n",
    "\n",
    "# 7. Modell laden\n",
    "model = TemporalFusionTransformer.load_from_checkpoint(\"/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/tft_20250515_153035.ckpt\")\n",
    "\n",
    "# 8. Vorhersage machen\n",
    "dataloader = prediction_dataset.to_dataloader(train=False, batch_size=1)\n",
    "predictions = model.predict(dataloader)\n",
    "\n",
    "# 9. Ergebnisse ausgeben\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Tag {i+1}: {float(pred[0]):.2f} Tage bis zum nächsten Jobwechsel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/predict.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training_dataset = torch.load(\"/Users/florianrunkel/Documents/02_Uni/04_Masterarbeit/masterthesis/backend/ml_pipe/models/tft/saved_models/training_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnung: Position 'Disponentin' konnte nicht gemappt werden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vorhersage für Tag 1:\n",
      "  Median: 0.5 Tage (0.0 Monate)\n",
      "  Untere Schranke: 0.5 Tage (0.0 Monate)\n",
      "  Obere Schranke: 0.6 Tage (0.0 Monate)\n",
      "  Unsicherheit: 0.1 Tage\n",
      "  Interpretation: Sehr wahrscheinlicher Jobwechsel innerhalb des nächsten Monats\n",
      "{'tag': 1, 'vorhersage': {'median': 0.5074630379676819, 'untere_schranke': 0.4871053397655487, 'obere_schranke': 0.5656402111053467, 'unsicherheit': 0.07853487133979797}}\n"
     ]
    }
   ],
   "source": [
    "from predict import read_linkedin_profile, predict_next_job_change\n",
    "\n",
    "# LinkedIn-Profil als String\n",
    "linkedin_data_str = r'''{\"skills\":[\"Multitasking\",\"Kundenservice\",\"Interpersonelle Fähigkeiten\",\"Kaltakquise\",\"Hubspot CRM\",\"Customer-Relationship-Management (CRM)\"],\"firstName\":\"Darya\",\"lastName\":\"Chernuska\",\"profilePicture\":\"https://media.licdn.com/dms/image/v2/D4E03AQE0yuZ6cg8f4A/profile-displayphoto-shrink_100_100/profile-displayphoto-shrink_100_100/0/1670856025914?e=1749686400&v=beta&t=jI1mkiVnkD7teWPncsg8QtKAwZKB-az53_4ny7C7XvI\",\"linkedinProfile\":\"https://www.linkedin.com/in/daryachernuska\",\"education\":[{\"duration\":\"01/01/2017 - 01/01/2022\",\"institution\":\"Ludwig-Maximilians-Universität München\",\"endDate\":\"01/01/2022\",\"degree\":\"\",\"startDate\":\"01/01/2017\"}],\"providerId\":\"ACoAAD0rz_IBI0XfqqBDUscwHoFwuOqJa_c5T2I\",\"workExperience\":[{\"duration\":\"01/03/2023 - Present\",\"endDate\":\"Present\",\"companyInformation\":{\"employee_count\":515,\"activities\":[\"Telefonie\",\"Internet\",\"Vernetzung\",\"Rechenzentrum\",\"Glasfaser\",\"Highspeed-Internet\",\"Business-Internet\",\"SIP-Trunk\",\"Cloud-Lösungen\",\"Connect-Cloud\",\"Connect-LAN\",\"Premium IP\",\"Internet + Telefonie\",\"Lösungen für Geschäftskunden\"],\"name\":\"M-net Telekommunikations GmbH\",\"description\":\"Als regionaler Telekommunikationsanbieter versorgt M-net große Teile Bayerns, den Großraum Ulm sowie weite Teile des hessischen Landkreises Main-Kinzig mit zukunftssicherer Kommunikationstechnologie.\",\"industry\":[\"Telecommunications\"]},\"description\":\"\",\"company\":\"M-net Telekommunikations GmbH\",\"location\":\"München, Bayern, Deutschland · Hybrid\",\"position\":\"Disponentin\",\"startDate\":\"01/03/2023\"},{\"duration\":\"01/08/2022 - 01/12/2022\",\"endDate\":\"01/12/2022\",\"companyInformation\":{\"employee_count\":2048,\"activities\":[\"HR Software\",\"HR Management\",\"Recruitung\",\"Employee Management\",\"Applicant Tracking System\",\"Employee Selfservice\",\"Time-Off Management\",\"Cloud Software\",\"Onboarding and Offboarding\",\"HR Reporting\",\"Performance Management\",\"Payroll\",\"HR\",\"HR Tech\",\"Human Resources\"],\"name\":\"Personio\",\"description\":\"Personio's Intelligent HR Platform helps small and medium-sized organizations unlock the power of people by making complicated, time-consuming tasks simple and efficient.\",\"industry\":[\"Software Development\"]},\"description\":\"\",\"company\":\"Personio\",\"location\":\"München, Bayern, Deutschland\",\"position\":\"Sales Development Representative\",\"startDate\":\"01/08/2022\"},{\"duration\":\"01/11/2017 - 01/07/2022\",\"endDate\":\"01/07/2022\",\"companyInformation\":{\"employee_count\":662,\"activities\":[\"Scandinavian design\",\"Furniture\",\"Design\",\"Product design\",\"Retail\",\"Web\",\"Steelcase partner\",\"Wholesale\",\"B2B\",\"Contract sales\",\"Online\",\"Digital\",\"Creativity\"],\"name\":\"BOLIA\",\"description\":\"Our collection is inspired by the vivid Scandinavian nature\",\"industry\":[\"Retail Furniture and Home Furnishings\"]},\"description\":\"\",\"company\":\"Bolia.com\",\"location\":\"München, Bayern, Deutschland\",\"position\":\"Sales Consultant\",\"startDate\":\"01/11/2017\"},{\"duration\":\"01/10/2015 - 01/11/2017\",\"endDate\":\"01/11/2017\",\"companyInformation\":{},\"description\":\"\",\"company\":\"Pepperminds\",\"location\":\"München, Bayern, Deutschland\",\"position\":\"Senior Team Lead\",\"startDate\":\"01/10/2015\"}],\"location\":\"Munich, Bavaria, Germany\",\"certifications\":[],\"headline\":\"-\",\"languageSkills\":{}}'''\n",
    "\n",
    "# Profil einlesen\n",
    "profile_data = read_linkedin_profile(linkedin_data_str)\n",
    "\n",
    "if profile_data:\n",
    "    # Vorhersage machen\n",
    "    predictions = predict_next_job_change(profile_data)\n",
    "    \n",
    "    if predictions:\n",
    "        # Ergebnisse ausgeben\n",
    "        for pred in predictions:\n",
    "            print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
