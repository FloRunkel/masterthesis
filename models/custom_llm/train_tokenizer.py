from tokenizers import ByteLevelBPETokenizer

# ðŸ”¹ Eigener Tokenizer mit Byte-Pair-Encoding (BPE)
tokenizer = ByteLevelBPETokenizer()

# ðŸ”¹ Beispieltexte zum Training des Tokenizers
texts = [
    "Hallo, wie geht es dir?",
    "KÃ¼nstliche Intelligenz verÃ¤ndert die Welt.",
    "Neuronale Netzwerke lernen aus Daten.",
]

# ðŸ”¹ Tokenizer trainieren
tokenizer.train_from_iterator(texts, vocab_size=5000, min_frequency=2)

# ðŸ”¹ Tokenizer speichern
tokenizer.save("models/custom_llm/llm/tokenizer.json")

print("Tokenizer erfolgreich erstellt und gespeichert!")
